{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Table of Contents**\n",
    "* [<span style=\"color:#A690A4\"> 0. Executive Summary](#exe_sum)\n",
    "* [<span style=\"color:#A690A4\"> 1. Introduction](#intro)\n",
    "* [<span style=\"color:#A690A4\"> 2. Collect, Wrangle & Explore Data](#process)\n",
    "* [<span style=\"color:#A690A4\"> 3. Predict Asking Price](#predict)\n",
    "* [<span style=\"color:#A690A4\"> 4. Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T01:42:48.828244Z",
     "iopub.status.busy": "2025-01-22T01:42:48.827812Z",
     "iopub.status.idle": "2025-01-22T01:42:48.835487Z",
     "shell.execute_reply": "2025-01-22T01:42:48.833664Z",
     "shell.execute_reply.started": "2025-01-22T01:42:48.828217Z"
    }
   },
   "source": [
    "# <span style=\"color:#5E6997\">Executive Summary</span> <a class=\"anchor\" id=\"exe_sum\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#5E6997\">Introduction</span> <a class=\"anchor\" id=\"intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining which features to analyze involves focusing on their **relevance to the target variable** (`HadHeartAttack`) while considering their statistical, medical, or logical relationship. Here's how you can prioritize features for analysis and preprocessing:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. **Direct Medical Relevance**\n",
    "   Features directly related to heart attack risk based on medical knowledge are high-priority. These include:\n",
    "   - **HadAngina**: Angina is a symptom of coronary heart disease, strongly associated with heart attacks.\n",
    "   - **HadStroke**: Stroke shares many risk factors with heart attacks.\n",
    "   - **HadCOPD**: Chronic obstructive pulmonary disease is associated with cardiovascular issues.\n",
    "   - **HadDiabetes**: Diabetes is a known risk factor for heart disease.\n",
    "   - **BMI, PhysicalActivities, SmokerStatus, AlcoholDrinkers**: Lifestyle factors strongly correlated with cardiovascular health.\n",
    "   - **AgeCategory, Sex**: Demographics highly predictive of heart disease.\n",
    "\n",
    "#### 2. **Comorbid Conditions**\n",
    "   Features that capture comorbidities or associated health issues:\n",
    "   - **HadDepressiveDisorder**: Depression is linked to heart disease through stress and inflammation.\n",
    "   - **HadKidneyDisease**: Kidney disease can contribute to heart conditions.\n",
    "   - **GeneralHealth**: Self-reported health status often reflects multiple underlying conditions.\n",
    "\n",
    "#### 3. **Behavioral and Preventive Health**\n",
    "   Features that reflect health-seeking behaviors or preventive measures:\n",
    "   - **LastCheckupTime**: Regular checkups may reduce heart attack risk through early intervention.\n",
    "   - **FluVaxLast12, PneumoVaxEver, TetanusLast10Tdap**: Proxy for health awareness or regular healthcare access.\n",
    "   - **ChestScan**: May indicate a history of lung or cardiovascular screening.\n",
    "\n",
    "#### 4. **Disabilities and Functional Limitations**\n",
    "   These features capture physical challenges that may be proxies for poor cardiovascular health:\n",
    "   - **DifficultyWalking**: A strong indicator of reduced mobility, often associated with heart conditions.\n",
    "   - **DifficultyConcentrating, DifficultyErrands**: Proxy for severe health or neurological issues.\n",
    "\n",
    "#### 5. **Statistical Considerations**\n",
    "   Features with significant variation or imbalance may require adjustments:\n",
    "   - **State**: May add little predictive power unless there’s geographic disparity in heart attack prevalence.\n",
    "   - **CovidPos**: Analyze if post-COVID complications affect heart attack risk.\n",
    "\n",
    "#### 6. **Feature Engineering Opportunities**\n",
    "   Create derived features or group similar ones:\n",
    "   - Create an **Exercise Regularity** feature from `PhysicalActivities` and `SleepHours`.\n",
    "   - Combine comorbidity indicators (`HadDiabetes`, `HadCOPD`, etc.) into a **Comorbidity Index**.\n",
    "\n",
    "#### 7. **Correlation with Target Variable**\n",
    "   Use a **correlation matrix** or statistical tests (e.g., chi-square for categorical variables, ANOVA for numeric variables) to identify features with the strongest relationships to `HadHeartAttack`.\n",
    "\n",
    "#### 8. **Class Imbalance**\n",
    "   Check if `HadHeartAttack` is imbalanced (e.g., many more \"No\" than \"Yes\"). If so, focus on features with strong predictive power to address the imbalance during modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#5E6997\">Collect, Wrangle, and Explore Data</span> <a class=\"anchor\" id=\"process\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:32.175804Z",
     "iopub.status.busy": "2025-02-12T17:42:32.175387Z",
     "iopub.status.idle": "2025-02-12T17:42:32.620903Z",
     "shell.execute_reply": "2025-02-12T17:42:32.619799Z",
     "shell.execute_reply.started": "2025-02-12T17:42:32.175770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*use_inf_as_na option is deprecated.*\")\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:32.622891Z",
     "iopub.status.busy": "2025-02-12T17:42:32.622425Z",
     "iopub.status.idle": "2025-02-12T17:42:35.835053Z",
     "shell.execute_reply": "2025-02-12T17:42:35.834249Z",
     "shell.execute_reply.started": "2025-02-12T17:42:32.622864Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = './heart_2022_no_nans.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:35.837428Z",
     "iopub.status.busy": "2025-02-12T17:42:35.837108Z",
     "iopub.status.idle": "2025-02-12T17:42:36.273818Z",
     "shell.execute_reply": "2025-02-12T17:42:36.272805Z",
     "shell.execute_reply.started": "2025-02-12T17:42:35.837401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Unique Values for Each Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:36.275959Z",
     "iopub.status.busy": "2025-02-12T17:42:36.275573Z",
     "iopub.status.idle": "2025-02-12T17:42:36.759246Z",
     "shell.execute_reply": "2025-02-12T17:42:36.758246Z",
     "shell.execute_reply.started": "2025-02-12T17:42:36.275932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Iterate through each column and process based on data type\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':  # Categorical variables\n",
    "        unique_values = df[column].unique()\n",
    "        print(f\"Column: {column}\")\n",
    "        print(f\"Unique Values: {unique_values}\\n\")\n",
    "    else:  # Numeric variables\n",
    "        col_min = df[column].min()\n",
    "        col_max = df[column].max()\n",
    "        print(f\"Column: {column}\")\n",
    "        print(f\"Range: {col_min} to {col_max}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Each Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a brief rundown of what each column in the dataset represents:\n",
    "\n",
    "`State` - State FIPS Code\\\n",
    "`Sex` - Sex of Respondent\\\n",
    "`GeneralHealth` - Would you say that in general your health is:\\\n",
    "`PhysicalHealthDays` - Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?\\\n",
    "`MentalHealthDays` - Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good?\\\n",
    "`LastCheckupTime` - About how long has it been since you last visited a doctor for a routine checkup?\\\n",
    "`PhysicalActivities` - During the past month, other than your regular job, did you participate in any physical activities or exercises such as running, calisthenics, golf, gardening, or walking for exercise?\\\n",
    "`SleepHours` - On average, how many hours of sleep do you get in a 24-hour period?\\\n",
    "`RemovedTeeth` - Not including teeth lost for injury or orthodontics, how many of your permanent teeth have been removed because of tooth decay or gum disease?\\\n",
    "`HadHeartAttack` - (Ever told) you had a heart attack, also called a myocardial infarction?\\\n",
    "`HadAngina` - (Ever told) (you had) angina or coronary heart disease?\\\n",
    "`HadStroke` - (Ever told) (you had) a stroke.\\\n",
    "`HadAsthma` - (Ever told) (you had) asthma?\\\n",
    "`HadSkinCancer` - (Ever told) (you had) skin cancer that is not melanoma?\\\n",
    "`HadCOPD` - (Ever told) (you had) C.O.P.D. (chronic obstructive pulmonary disease), emphysema or chronic bronchitis?\\\n",
    "`HadDepressiveDisorder` - (Ever told) (you had) a depressive disorder (including depression, major depression, dysthymia, or minor depression)?\\\n",
    "`HadKidneyDisease` - Not including kidney stones, bladder infection or incontinence, were you ever told you had kidney disease?\\\n",
    "`HadArthritis` - (Ever told) (you had) some form of arthritis, rheumatoid arthritis, gout, lupus, or fibromyalgia?  (Arthritis diagnoses include: rheumatism, polymyalgia rheumatica; osteoarthritis (not osteporosis); tendonitis, bursitis, bunion, tennis elbow; carpal tunnel syndrome, tarsal tunnel syndrome; joint infection, etc.)\\\n",
    "`HadDiabetes` - (Ever told) (you had) diabetes?\\\n",
    "`DeafOrHardOfHearing` - Are you deaf or do you have serious difficulty hearing?\\\n",
    "`BlindOrVisionDifficulty` - Are you blind or do you have serious difficulty seeing, even when wearing glasses?\\\n",
    "`DifficultyConcentrating` - Because of a physical, mental, or emotional condition, do you have serious difficulty concentrating, remembering, or making decisions?\\\n",
    "`DifficultyWalking` - Do you have serious difficulty walking or climbing stairs?\\\n",
    "`DifficultyDressingBathing` - Do you have difficulty dressing or bathing?\\\n",
    "`DifficultyErrands` - Because of a physical, mental, or emotional condition, do you have difficulty doing errands alone such as visiting a doctor´s office or shopping?\\\n",
    "`SmokerStatus` - Four-level smoker status:  Everyday smoker, Someday smoker, Former smoker, Non-smoker\\\n",
    "`ECigaretteUsage` - Would you say you have never used e-cigarettes or other electronic vaping products in your entire life or now use them every day, use them some days, or used them in the past but do not currently use them at all?\\\n",
    "`ChestScan` - Have you ever had a CT or CAT scan of your chest area?\\\n",
    "`RaceEthnicityCategory` - Five-level race/ethnicity category\\\n",
    "`AgeCategory` - Fourteen-level age category\\\n",
    "`HeightInMeters` - Reported height in meters\\\n",
    "`WeightInKilograms` - Reported weight in kilograms\\\n",
    "`BMI` - Body Mass Index (BMI)\\\n",
    "`AlchoholDrinkers` - Adults who reported having had at least one drink of alcohol in the past 30 days.\\\n",
    "`HIVTesting` - Adults who have ever been tested for HIV\\\n",
    "`FluVaxLast12` - During the past 12 months, have you had either flu vaccine that was sprayed in your nose or flu shot injected into your arm?\\\n",
    "`PneumoVaxEver` - Have you ever had a pneumonia shot also known as a pneumococcal vaccine?\\\n",
    "`TetanusLast10Tdap` - Have you received a tetanus shot in the past 10 years? Was this Tdap, the tetanus shot that also has pertussis or whooping cough vaccine?\\\n",
    "`HighRiskLastYear` - You have injected any drug other than those prescribed for you in the past year. You have been treated for a sexually transmitted disease or STD in the past year. You have given or received money or drugs in exchange for sex in the past year.\\\n",
    "`CovidPos` - Has a doctor, nurse, or other health professional ever told you that you tested positive for COVID 19?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:36.760583Z",
     "iopub.status.busy": "2025-02-12T17:42:36.760263Z",
     "iopub.status.idle": "2025-02-12T17:42:36.764794Z",
     "shell.execute_reply": "2025-02-12T17:42:36.763524Z",
     "shell.execute_reply.started": "2025-02-12T17:42:36.760557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:36.766243Z",
     "iopub.status.busy": "2025-02-12T17:42:36.765824Z",
     "iopub.status.idle": "2025-02-12T17:42:36.881741Z",
     "shell.execute_reply": "2025-02-12T17:42:36.880688Z",
     "shell.execute_reply.started": "2025-02-12T17:42:36.766213Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Select categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Distribution of Unique Values for each Categorical Variable¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some graphs have been oriented horizontally to make room for their Category names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:36.883193Z",
     "iopub.status.busy": "2025-02-12T17:42:36.882772Z",
     "iopub.status.idle": "2025-02-12T17:42:45.304810Z",
     "shell.execute_reply": "2025-02-12T17:42:45.303457Z",
     "shell.execute_reply.started": "2025-02-12T17:42:36.883159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# List of columns to orient horizontally\n",
    "horizontal_columns = [\n",
    "    'State', 'LastCheckupTime', 'HadDiabetes', \n",
    "    'SmokerStatus', 'ECigaretteUsage',  \n",
    "    'RaceEthnicityCategory', 'AgeCategory', 'TetanusLast10Tdap', 'CovidPos'\n",
    "]\n",
    "\n",
    "# Number of columns per row for the grid\n",
    "plots_per_row = 2\n",
    "\n",
    "# Calculate the number of rows needed\n",
    "num_categories = len(categorical_columns)\n",
    "num_rows = math.ceil(num_categories / plots_per_row)\n",
    "\n",
    "# Create a grid of subplots\n",
    "fig, axes = plt.subplots(num_rows, plots_per_row, figsize=(15, num_rows * 5))\n",
    "\n",
    "# Flatten axes array if more than one row, or wrap in a list if only one row\n",
    "if num_rows == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "# Plot the distribution of unique values for each categorical variable\n",
    "for idx, column in enumerate(categorical_columns):\n",
    "    value_counts = df[column].value_counts()\n",
    "    \n",
    "    # Check if the column should be horizontal\n",
    "    if column in horizontal_columns:\n",
    "        sns.barplot(y=value_counts.index, x=value_counts.values, ax=axes[idx])  # Horizontal bar plot\n",
    "        axes[idx].set_ylabel('Categories', fontsize=10)\n",
    "        axes[idx].set_xlabel('Count', fontsize=10)\n",
    "    else:\n",
    "        sns.barplot(x=value_counts.index, y=value_counts.values, ax=axes[idx])  # Vertical bar plot\n",
    "        axes[idx].set_xlabel('Categories', fontsize=10)\n",
    "        axes[idx].set_ylabel('Count', fontsize=10)\n",
    "    \n",
    "    # Set title and tick sizes\n",
    "    axes[idx].set_title(f'Distribution in {column}', fontsize=12)\n",
    "    axes[idx].tick_params(axis='y', labelsize=8)\n",
    "    axes[idx].tick_params(axis='x', labelsize=8)\n",
    "\n",
    "# Hide unused subplots if any\n",
    "for idx in range(len(categorical_columns), len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Feature Improvements Based on Graphs \n",
    "Improving the performance of features in the models can be achieved by addressing issues related to data quality, feature engineering, and preprocessing. Here are some specific ideas for the features:\n",
    "\n",
    "\n",
    "**1. ✅ Address Imbalanced Distributions**\n",
    "For features where one category dominates (e.g., health conditions like `HadHeartAttack`, `HadStroke`, `DifficultyWalking`):\n",
    "- **Rebalancing Techniques:**\n",
    "  - ✅ Use techniques like undersampling the majority class.\n",
    "  - Assign higher class weights in models to penalize misclassification of the minority class.\n",
    "- **Feature Transformation:**\n",
    "  - ✅ Aggregate similar categories into a binary or reduced set of classes (e.g., combine categories in `HadDiabetes` into \"Yes\" vs. \"No\" groups).\n",
    "\n",
    "\n",
    "**2. ✅ Handle Categorical Features Effectively**\n",
    "To improve their predictive power:\n",
    "- **Encoding Methods:**\n",
    "  - ✅ Use **one-hot encoding** for nominal categories (e.g., `RaceEthnicityCategory` or `State`).\n",
    "  - ✅ Apply **ordinal encoding** for ordinal features (e.g., `GeneralHealth` and `AgeCategory`).\n",
    "  - ✅ Experiment with **target encoding** for high cardinality features like `State` if there is a clear relationship with the target variable.\n",
    "- **Feature Reduction:**\n",
    "  - ✅ For high-cardinality features (e.g., `State`), consider grouping less frequent categories into an \"Other\" category to reduce noise and sparsity.\n",
    "\n",
    "\n",
    "**3. ✅ Handle Missing and Rare Categories**\n",
    "Rare categories in features like `ECigaretteUsage` and `TetanusLast10Tdap` can create noise:\n",
    "- ✅ Combine rare categories or reassign them to a broader category (e.g., \"Used occasionally\").\n",
    "\n",
    "\n",
    "**4. ✅ Feature Interactions**\n",
    "Certain features may not perform well on their own but could have a strong impact when combined:\n",
    "- **Create Interaction Features:**\n",
    "  - ✅ Combine related features (e.g., `PhysicalActivities` and `GeneralHealth`) to capture interdependencies.\n",
    "  - ✅ Use domain knowledge to create meaningful interactions (e.g., `AgeCategory` × `HadHeartAttack`).\n",
    "\n",
    "\n",
    "**5. ✅ Feature Scaling and Transformation**\n",
    "For models sensitive to feature scaling (e.g., logistic regression or SVMs):\n",
    "- ✅ Apply scaling techniques like standardization or min-max scaling to features like `AgeCategory`.\n",
    "\n",
    "\n",
    "**6. ✅ Feature Selection**\n",
    "Identify the most relevant features for your model to reduce noise:\n",
    "- ✅ Use feature importance scores from tree-based models (e.g., Random Forest, XGBoost) or statistical tests (e.g., chi-squared for categorical features).\n",
    "- ✅ Perform dimensionality reduction techniques like PCA or mutual information to identify and retain the most important features.\n",
    "\n",
    "\n",
    "**7. ✅ Address Potential Multicollinearity**\n",
    "Some features may overlap conceptually (e.g., `GeneralHealth` and `PhysicalActivities` or `SmokerStatus` and `ECigaretteUsage`). This can negatively impact models like logistic regression:\n",
    "- ✅ Use correlation matrices or variance inflation factor (VIF) analysis to identify multicollinearity.\n",
    "- ✅ Drop one of the redundant features if necessary or combine them.\n",
    "\n",
    "\n",
    "**8. ✅ Model-Specific Approaches**\n",
    "Optimize features based on the model you are using:\n",
    "- ✅ **Tree-Based Models (e.g., XGBoost, LightGBM):**\n",
    "  - These models handle categorical data and imbalances well but may benefit from better binning or interaction terms.\n",
    "- ✅ **Linear Models (e.g., Logistic Regression, SVM):**\n",
    "  - Ensure proper scaling and encoding.\n",
    "  - Experiment with regularization (L1 or L2) to reduce overfitting.\n",
    "- ✅ **Neural Networks:**\n",
    "  - Normalize inputs, and use embedding layers for high-cardinality features like `State`.\n",
    "\n",
    "\n",
    "**9. ✅ Domain-Specific Improvements**\n",
    "Leverage domain knowledge to create features that are more predictive:\n",
    "- ✅ Combine time-related features like `LastCheckupTime` with `GeneralHealth` to create a \"Proactive Health Score.\"\n",
    "- ✅ Use population-weighted state data (e.g., average health index for each state) as a new feature.\n",
    "\n",
    "\n",
    "By applying these strategies, I can improve the predictive power and performance of the features in the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:45.308417Z",
     "iopub.status.busy": "2025-02-12T17:42:45.307901Z",
     "iopub.status.idle": "2025-02-12T17:42:45.318956Z",
     "shell.execute_reply": "2025-02-12T17:42:45.317757Z",
     "shell.execute_reply.started": "2025-02-12T17:42:45.308387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Select numerical columns\n",
    "numerical_columns = df.select_dtypes(include=['number']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:45.321513Z",
     "iopub.status.busy": "2025-02-12T17:42:45.321189Z",
     "iopub.status.idle": "2025-02-12T17:42:53.749967Z",
     "shell.execute_reply": "2025-02-12T17:42:53.748942Z",
     "shell.execute_reply.started": "2025-02-12T17:42:45.321483Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of each numerical variable and save each graph as a PNG file\n",
    "for column in numerical_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[column], kde=True, bins=30, color='skyblue')\n",
    "    plt.title(f'Distribution of {column}', fontsize=14)\n",
    "    plt.xlabel(column, fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot to a PNG file\n",
    "    # filename = f'num_dist_{column}.png'\n",
    "    # plt.savefig(filename, dpi=300)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Feature Improvements Based on Graphs\n",
    "\n",
    "1. ✅ **WeightInKilograms**\n",
    "   - **Distribution**: The distribution appears to be unimodal and slightly right-skewed.\n",
    "   - **Actions**:\n",
    "     - ✅ **Normalization or Standardization**: Apply normalization (min-max scaling) or standardization (z-score scaling) to ensure the weight feature is on a similar scale as others in your dataset.\n",
    "     - ✅ **Outlier Handling**: Inspect for outliers (e.g., weights above 200kg). Consider capping or transforming extreme values to reduce their impact on the model.\n",
    "\n",
    "2. ✅ **SleepHours**\n",
    "   - **Distribution**: The distribution shows multiple spikes, which could indicate certain discrete values (e.g., 4, 6, 8 hours) are common.\n",
    "   - **Actions**:\n",
    "     - ✅ **Transform to Categorical**: Consider binning the sleep hours into categories such as \"Low sleep (<6)\", \"Normal sleep (6-8)\", \"High sleep (>8)\" for models that handle categorical data well.\n",
    "     - ✅ **Feature Engineering**: Calculate a \"sleep deficit\" feature (e.g., ideal sleep hours minus actual sleep hours) if there’s an expected benchmark for the population.\n",
    "\n",
    "3. ✅ **PhysicalHealthDays**\n",
    "   - **Distribution**: Highly right-skewed, with a large proportion of data at zero.\n",
    "   - **Actions**:\n",
    "     - ✅ **Log Transformation**: Apply a log or square root transformation to reduce skewness and compress the range of large values.\n",
    "     - ✅ **Categorization**: Group values into categories like \"No issues (0 days)\", \"Mild issues (1-7 days)\", \"Moderate issues (8-14 days)\", and \"Severe issues (15+ days)\" to simplify the distribution.\n",
    "     - ✅ **Feature Engineering**: Combine with \"MentalHealthDays\" to create a total health impact variable.\n",
    "\n",
    "4. ✅ **MentalHealthDays**\n",
    "   - **Distribution**: Similar to PhysicalHealthDays, with a large proportion of zeros and a right-skewed tail.\n",
    "   - **Actions**:\n",
    "     - ✅ **Transformation**: Use log transformation to handle skewness.\n",
    "     - ✅ **Binary Encoding**: Create an indicator feature for \"Had any mental health issues\" (1 if > 0 days, 0 otherwise).\n",
    "\n",
    "5. ✅ **HeightInMeters**\n",
    "   - **Distribution**: Symmetrical, resembling a normal distribution.\n",
    "   - **Actions**:\n",
    "     - ✅ **Standardization**: Normalize or standardize to align scales across features.\n",
    "\n",
    "6. ✅ **BMI**\n",
    "   - **Distribution**: Right-skewed, with a concentration around the 20-30 range.\n",
    "   - **Actions**:\n",
    "     - ✅ **Feature Engineering**: Interact BMI with other health-related features (e.g., physical activity, chronic conditions) to identify combined effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features Graphed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:53.751208Z",
     "iopub.status.busy": "2025-02-12T17:42:53.750902Z",
     "iopub.status.idle": "2025-02-12T17:42:58.226856Z",
     "shell.execute_reply": "2025-02-12T17:42:58.225719Z",
     "shell.execute_reply.started": "2025-02-12T17:42:53.751182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensure 'HadHeartAttack' is present in the dataset\n",
    "target = 'HadHeartAttack'\n",
    "\n",
    "# Define a subset of key features for visualization (based on prior analysis)\n",
    "key_features = [\n",
    "    'AgeCategory', 'Sex', 'BMI', 'SmokerStatus', 'PhysicalActivities',\n",
    "    'GeneralHealth', 'HadAngina', 'HadStroke', 'HadDiabetes', 'AlcoholDrinkers'\n",
    "]\n",
    "\n",
    "# Visualize feature-target relationships\n",
    "for feature in key_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    if df[feature].dtype == 'object':  # Categorical features\n",
    "        sns.countplot(data=df, x=feature, hue=target, order=df[feature].value_counts().index)\n",
    "        plt.title(f'{feature} vs {target}', fontsize=14)\n",
    "        plt.xlabel(feature, fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "    else:  # Numeric features\n",
    "        sns.boxplot(data=df, x=target, y=feature)\n",
    "        plt.title(f'{feature} Distribution by {target}', fontsize=14)\n",
    "        plt.xlabel(target, fontsize=12)\n",
    "        plt.ylabel(feature, fontsize=12)\n",
    "    \n",
    "    plt.xticks(rotation=45, fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to a PNG file\n",
    "    # filename = f'{feature}_vs_HadHeartAttack.png'\n",
    "    # plt.savefig(filename, dpi=300)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Feature Improvements Based on Graphs\n",
    "\n",
    "Analyzing these visualizations, here are the feature-specific observations and recommendations to improve the graphs and their utility in modeling:\n",
    "\n",
    "1. ✅ **SmokerStatus vs HadHeartAttack**: Smokers, particularly current smokers, show a higher proportion of heart attacks compared to non-smokers.\n",
    "     - ✅ Consider encoding this variable into ordinal categories such as `Never Smoked (0)`, `Former Smoker (1)`, `Current Smoker (2)`.\n",
    "\n",
    "2. ✅ **Sex vs HadHeartAttack**: There is a difference in heart attack occurrences between males and females, with males showing a slightly higher proportion.\n",
    "     - ✅ Ensure balanced sampling across genders if the dataset is imbalanced.\n",
    "\n",
    "3. ✅ **PhysicalActivities vs HadHeartAttack**: Individuals who do not engage in physical activities appear to have a higher proportion of heart attacks.\n",
    "     - ✅ Keep this variable as a binary flag. \n",
    "     - ✅ Create an interaction term between this feature and other health-related metrics (e.g., BMI or AgeCategory) for potentially better prediction.\n",
    "\n",
    "4. **AlcoholDrinkers vs HadHeartAttack**: There is no significant difference in heart attack occurrences between drinkers and non-drinkers.\n",
    "     - If this variable doesn't improve the model's performance, consider excluding it or combining it with other lifestyle features for composite effects.\n",
    "\n",
    "\n",
    "These strategies should help in improving the predictive power of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:58.227988Z",
     "iopub.status.busy": "2025-02-12T17:42:58.227674Z",
     "iopub.status.idle": "2025-02-12T17:42:58.235066Z",
     "shell.execute_reply": "2025-02-12T17:42:58.233723Z",
     "shell.execute_reply.started": "2025-02-12T17:42:58.227964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a new feature 'AgesGrouped' as a copy of 'AgeCategory'\n",
    "df['AgesGrouped'] = df['AgeCategory']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:58.236450Z",
     "iopub.status.busy": "2025-02-12T17:42:58.236113Z",
     "iopub.status.idle": "2025-02-12T17:42:58.547799Z",
     "shell.execute_reply": "2025-02-12T17:42:58.546539Z",
     "shell.execute_reply.started": "2025-02-12T17:42:58.236425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define ordinal mappings for each feature\n",
    "ordinal_mappings = {\n",
    "    \"GeneralHealth\": {\n",
    "        'Poor': 1,\n",
    "        'Fair': 2,\n",
    "        'Good': 3,\n",
    "        'Very good': 4,\n",
    "        'Excellent': 5\n",
    "    },\n",
    "    \"LastCheckupTime\": {\n",
    "        '5 or more years ago': 1,\n",
    "        'Within past 5 years (2 years but less than 5 years ago)': 2,\n",
    "        'Within past 2 years (1 year but less than 2 years ago)': 3,\n",
    "        'Within past year (anytime less than 12 months ago)': 4\n",
    "    },\n",
    "    \"RemovedTeeth\": {\n",
    "        'None of them': 0,\n",
    "        '1 to 5': 1,\n",
    "        '6 or more, but not all': 2,\n",
    "        'All': 3\n",
    "    },\n",
    "    \"HadDiabetes\": {\n",
    "        'No': 0,\n",
    "        'No, pre-diabetes or borderline diabetes': 0,\n",
    "        'Yes, but only during pregnancy (female)': 1,\n",
    "        'Yes': 1\n",
    "    },\n",
    "    \"SmokerStatus\": {\n",
    "        'Never smoked': 0,\n",
    "        'Former smoker': 1,\n",
    "        'Current smoker - now smokes some days': 2,\n",
    "        'Current smoker - now smokes every day': 2 # Combine these two.\n",
    "    },\n",
    "    \"ECigaretteUsage\": {\n",
    "        'Never used e-cigarettes in my entire life': 0,\n",
    "        'Not at all (right now)': 1,\n",
    "        'Use them some days': 2,\n",
    "        'Use them every day': 2 # Combine these two\n",
    "    },\n",
    "    \"AgeCategory\": {\n",
    "        'Age 18 to 24': 1,\n",
    "        'Age 25 to 29': 2,\n",
    "        'Age 30 to 34': 3,\n",
    "        'Age 35 to 39': 4,\n",
    "        'Age 40 to 44': 5,\n",
    "        'Age 45 to 49': 6,\n",
    "        'Age 50 to 54': 7,\n",
    "        'Age 55 to 59': 8,\n",
    "        'Age 60 to 64': 9,\n",
    "        'Age 65 to 69': 10,\n",
    "        'Age 70 to 74': 11,\n",
    "        'Age 75 to 79': 12,\n",
    "        'Age 80 or older': 13\n",
    "    },\n",
    "    \"TetanusLast10Tdap\": {\n",
    "        'No, did not receive any tetanus shot in the past 10 years': 1,\n",
    "        'Yes, received tetanus shot but not sure what type': 2,\n",
    "        'Yes, received tetanus shot, but not Tdap': 2, # Combined these two\n",
    "        'Yes, received Tdap': 3\n",
    "    },\n",
    "    \"CovidPos\": {\n",
    "        'No': 0,\n",
    "        'Yes': 1,\n",
    "        'Tested positive using home test without a health professional': 1 # Combined these two\n",
    "    }\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding to the specified features\n",
    "for column, mapping in ordinal_mappings.items():\n",
    "    df[column] = df[column].map(mapping)\n",
    "\n",
    "# Verify the changes\n",
    "print(df[list(ordinal_mappings.keys())].head())\n",
    "\n",
    "# Remove `AgeCategory'\n",
    "df = df.drop(columns=['AgeCategory'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:58.549410Z",
     "iopub.status.busy": "2025-02-12T17:42:58.548991Z",
     "iopub.status.idle": "2025-02-12T17:42:59.140472Z",
     "shell.execute_reply": "2025-02-12T17:42:59.139372Z",
     "shell.execute_reply.started": "2025-02-12T17:42:58.549369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Excluded features for ordinal encoding\n",
    "excluded_features = [\n",
    "    'GeneralHealth',\n",
    "    'LastCheckupTime',\n",
    "    'RemovedTeeth',\n",
    "    'HadDiabetes',\n",
    "    'SmokerStatus',\n",
    "    'ECigaretteUsage',\n",
    "    'AgeGrouped',\n",
    "    'AgeCategory',\n",
    "    'State',\n",
    "    'TetanusLast10Tdap'\n",
    "]\n",
    "\n",
    "# Identify features to one-hot encode\n",
    "one_hot_features = [col for col in categorical_columns if col not in excluded_features]\n",
    "\n",
    "# One-hot encode the selected features and ensure the output is 0/1\n",
    "df = pd.get_dummies(df, columns=one_hot_features, drop_first=True)\n",
    "\n",
    "# Rename binary one-hot encoded columns to match the original feature names\n",
    "for feature in one_hot_features:\n",
    "    yes_col = f\"{feature}_Yes\"  # Check for \"_Yes\" columns\n",
    "    if yes_col in df.columns:\n",
    "        df.rename(columns={yes_col: feature}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:59.141921Z",
     "iopub.status.busy": "2025-02-12T17:42:59.141531Z",
     "iopub.status.idle": "2025-02-12T17:42:59.179345Z",
     "shell.execute_reply": "2025-02-12T17:42:59.178225Z",
     "shell.execute_reply.started": "2025-02-12T17:42:59.141890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert all boolean values in the DataFrame to integers (0/1)\n",
    "df = df.astype({col: int for col in df.select_dtypes(include=['bool']).columns})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:59.181036Z",
     "iopub.status.busy": "2025-02-12T17:42:59.180711Z",
     "iopub.status.idle": "2025-02-12T17:42:59.221457Z",
     "shell.execute_reply": "2025-02-12T17:42:59.220415Z",
     "shell.execute_reply.started": "2025-02-12T17:42:59.181000Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=['HadHeartAttack'])  \n",
    "y = df['HadHeartAttack'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:59.223162Z",
     "iopub.status.busy": "2025-02-12T17:42:59.222687Z",
     "iopub.status.idle": "2025-02-12T17:42:59.849354Z",
     "shell.execute_reply": "2025-02-12T17:42:59.848366Z",
     "shell.execute_reply.started": "2025-02-12T17:42:59.223121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 Feature Descriptions\n",
    "1. **`ComorbidityIndex`** → Counts the number of chronic conditions a person has (e.g., Angina, Stroke, Diabetes, COPD, etc.).  \n",
    "2. **`AgeMidpoint`** → Converts the age category into a numeric midpoint for easier analysis.  \n",
    "3. **`ExerciseRegularity`** → Combines physical activity participation with sleep hours to estimate overall exercise regularity.  \n",
    "4. **`HealthAwarenessIndex`** → Counts the number of preventive health measures taken (e.g., flu vaccine, HIV testing).  \n",
    "5. **`ObesityClass`** → Categorizes BMI into standard obesity classes (`0` = Underweight, `0.333` = Normal weight, `0.666` = Overweight, `1` = Obese).  \n",
    "6. **`HeartRiskScore`** → Counts the number of heart risk factors (Angina, Stroke, Difficulty Walking).  \n",
    "7. **`SleepDeficit`** → Measures how much sleep deviates from the ideal 8 hours.  \n",
    "8. **`PhysicalHealthInteraction`** → Interaction term multiplying `PhysicalActivities` by `GeneralHealth`, capturing how activity affects perceived health.  \n",
    "9. **`ComorbidityHealthInteraction`** → Interaction between `ComorbidityIndex` and `GeneralHealth` to assess how chronic conditions impact health perception.  \n",
    "10. **`ExerciseObesityInteraction`** → Interaction between `ExerciseRegularity` and `ObesityClass`, capturing the relationship between physical activity and weight category.  \n",
    "11. **`HeartSleepInteraction`** → Interaction between `HeartRiskScore` and `SleepDeficit`, capturing how heart risk factors relate to sleep patterns.  \n",
    "12. **`ComorbidityObesityInteraction`** → Interaction between `ComorbidityIndex` and `ObesityClass`, highlighting the correlation between chronic conditions and weight.  \n",
    "13. **`HeartHealthInteraction`** → Interaction between `HeartRiskScore` and `GeneralHealth`, assessing how heart risk impacts self-reported health status.  \n",
    "14. **`PhysicalAgeInteraction`** → Interaction between `PhysicalActivities` and `AgeMidpoint`, examining how age influences physical activity habits.  \n",
    "15. **`ProactiveHealthScore`** → Interaction between `LastCheckupTime` and `GeneralHealth`, capturing how proactive individuals are about their healthcare.  \n",
    "16. **`StateHealthIndex`** → Population-weighted health score based on the average `GeneralHealth` of individuals in a given state.  \n",
    "17. **`SleepCategory`** → Categorizes sleep hours into `\"Low sleep\" (<6 hours)`, `\"Normal sleep\" (6-8 hours)`, and `\"High sleep\" (>8 hours)`.  \n",
    "18. **`PhysicalHealthCategory`** → Categorizes `PhysicalHealthDays` into `\"No issues\" (0 days)`, `\"Mild\" (1-7 days)`, `\"Moderate\" (8-14 days)`, and `\"Severe\" (15+ days)`.  \n",
    "19. **`TotalHealthDays`** → Sum of `PhysicalHealthDays` and `MentalHealthDays`, representing total days of health issues.  \n",
    "20. **`HadMentalHealthIssues`** → **Binary feature** (`1` if `MentalHealthDays` > 0, otherwise `0`), identifying individuals with any mental health concerns.  \n",
    "21. **`BMI_PhysicalActivity`** → Interaction between **BMI and Physical Activity**, assessing how obesity affects exercise participation.  \n",
    "22. **`BMI_ChronicConditions`** → Interaction between **BMI and ComorbidityIndex**, identifying the relationship between obesity and chronic disease burden.  \n",
    "23. **`BMI_SleepDeficit`** → Interaction between **BMI and SleepDeficit**, capturing how sleep disturbances may be associated with weight.  \n",
    "24. **`BMI_HeartRisk`** → Interaction between **BMI and HeartRiskScore**, evaluating the combined impact of obesity and heart-related risks.\n",
    "25. **`BMI_SleepDeficit`** → Measures the combined effect of BMI and Sleep Deficit, identifying potential links between weight and insufficient sleep.  \n",
    "26. **`BMI_HeartRisk`** → Captures the relationship between BMI and Heart Risk Score, assessing the joint impact of weight and cardiovascular conditions.  \n",
    "27. **`PhysicalActivity_Age`** → Examines how physical activity varies with age, indicating exercise trends across different age groups.  \n",
    "28. **`PhysicalActivity_BMI`** → Explores the relationship between BMI and exercise participation, highlighting how weight influences activity levels.  \n",
    "29. **`PhysicalActivity_ChronicConditions`** → Assesses how chronic conditions affect physical activity levels, identifying potential exercise barriers.  \n",
    "30. **`PhysicalActivity_SleepDeficit`** → Captures the interaction between physical activity and sleep deficit, linking exercise habits to sleep patterns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:42:59.850962Z",
     "iopub.status.busy": "2025-02-12T17:42:59.850548Z",
     "iopub.status.idle": "2025-02-12T17:43:41.317226Z",
     "shell.execute_reply": "2025-02-12T17:43:41.315963Z",
     "shell.execute_reply.started": "2025-02-12T17:42:59.850932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def feature_engineering(data):\n",
    "    # 1. Comorbidity Index\n",
    "    chronic_conditions = [\n",
    "        'HadAngina', 'HadStroke', 'HadCOPD', 'HadDepressiveDisorder', \n",
    "        'HadKidneyDisease', 'HadArthritis', 'HadDiabetes'\n",
    "    ]\n",
    "    data['ComorbidityIndex'] = data[chronic_conditions].apply(lambda x: x.eq('Yes').sum(), axis=1)\n",
    "\n",
    "    # 2. Apply Square Root Transformation to Reduce Skewness\n",
    "    skewed_features = ['ComorbidityIndex', 'PhysicalHealthDays', 'MentalHealthDays']\n",
    "    for feature in skewed_features:\n",
    "        data[feature] = np.sqrt(data[feature])\n",
    "    \n",
    "    # 3. Age Group Numeric\n",
    "    age_map = {\n",
    "        'Age 18 to 24': 21, 'Age 25 to 29': 27, 'Age 30 to 34': 32,\n",
    "        'Age 35 to 39': 37, 'Age 40 to 44': 42, 'Age 45 to 49': 47,\n",
    "        'Age 50 to 54': 52, 'Age 55 to 59': 57, 'Age 60 to 64': 62,\n",
    "        'Age 65 to 69': 67, 'Age 70 to 74': 72, 'Age 75 to 79': 77,\n",
    "        'Age 80 or older': 85\n",
    "    }\n",
    "    data['AgeMidpoint'] = data['AgesGrouped'].map(age_map)\n",
    "\n",
    "    # 4. Exercise Regularity\n",
    "    data['ExerciseRegularity'] = data['PhysicalActivities'].apply(lambda x: 1 if x == 'Yes' else 0) * data['SleepHours']\n",
    "    data['ExerciseRegularity'] = data['ExerciseRegularity'].replace([float('inf'), -float('inf')], float('nan'))\n",
    "    data['ExerciseRegularity'] = data['ExerciseRegularity'].fillna(0)\n",
    "\n",
    "    # 5. Health Awareness Index\n",
    "    preventive_measures = ['FluVaxLast12', 'PneumoVaxEver', 'TetanusLast10Tdap', 'HIVTesting']\n",
    "    data['HealthAwarenessIndex'] = data[preventive_measures].apply(lambda x: x.eq('Yes').sum(), axis=1)\n",
    "\n",
    "    # 6. Obesity Class\n",
    "    def classify_bmi(bmi):\n",
    "        if bmi < 18.5:\n",
    "            return 0\n",
    "        elif 18.5 <= bmi < 24.9:\n",
    "            return 0.33333\n",
    "        elif 25 <= bmi < 29.9:\n",
    "            return 0.66666\n",
    "        else:\n",
    "            return 1\n",
    "    data['ObesityClass'] = data['BMI'].apply(classify_bmi)\n",
    "\n",
    "    # 7. Heart Risk Score \n",
    "    heart_risk_factors = ['HadAngina', 'HadStroke', 'DifficultyWalking']\n",
    "    data['HeartRiskScore'] = data[heart_risk_factors].sum(axis=1) \n",
    "\n",
    "    # 8. Apply Square Root Transformation to Reduce Skewness \n",
    "    skewed_features = ['HeartRiskScore', 'BMI']\n",
    "    for feature in skewed_features:\n",
    "        data[feature] = np.sqrt(data[feature])\n",
    "\n",
    "    # 9. Sleep Deficit \n",
    "    IDEAL_SLEEP_HOURS = 8\n",
    "    data['SleepDeficit'] = IDEAL_SLEEP_HOURS - data['SleepHours']  \n",
    "\n",
    "    # 10. Interaction Feature: PhysicalActivities × GeneralHealth\n",
    "    data['PhysicalHealthInteraction'] = (\n",
    "        data['PhysicalActivities'].apply(lambda x: 1 if x == 'Yes' else 0) * data['GeneralHealth']\n",
    "    )\n",
    "\n",
    "    # 11. ComorbidityIndex × GeneralHealth\n",
    "    data['ComorbidityHealthInteraction'] = data['ComorbidityIndex'] * data['GeneralHealth']\n",
    "\n",
    "    # 12. ExerciseRegularity × ObesityClass\n",
    "    data['ExerciseObesityInteraction'] = data['ExerciseRegularity'] * data['ObesityClass']\n",
    "\n",
    "    # 13. HeartRiskScore × SleepDeficit\n",
    "    data['HeartSleepInteraction'] = data['HeartRiskScore'] * data['SleepDeficit']\n",
    "\n",
    "    # 14. **ComorbidityIndex × ObesityClass**  \n",
    "    data['ComorbidityObesityInteraction'] = data['ComorbidityIndex'] * data['ObesityClass']\n",
    "\n",
    "    # 15. **HeartRiskScore × GeneralHealth**  \n",
    "    data['HeartHealthInteraction'] = data['HeartRiskScore'] * data['GeneralHealth']\n",
    "\n",
    "    # 16. **PhysicalActivities × AgeMidpoint**  \n",
    "    data['PhysicalAgeInteraction'] = data['PhysicalActivities'].apply(lambda x: 1 if x == 'Yes' else 0) * data['AgeMidpoint']\n",
    "\n",
    "    # 17. **Proactive Health Score: Pre-Encoded LastCheckupTime × GeneralHealth**\n",
    "    data['ProactiveHealthScore'] = data['LastCheckupTime'] * data['GeneralHealth']\n",
    "\n",
    "    # 18. **Population-Weighted State Health Index**\n",
    "    state_health_index = data.groupby(\"State\")[\"GeneralHealth\"].mean().to_dict()\n",
    "    data[\"StateHealthIndex\"] = data[\"State\"].map(state_health_index)\n",
    "\n",
    "    # 19. **Categorize Sleep Hours into \"Low sleep\", \"Normal sleep\", and \"High sleep\"**\n",
    "    def categorize_sleep_hours(hours):\n",
    "        if hours < 6:\n",
    "            return 0\n",
    "        elif 6 <= hours <= 8:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    data['SleepCategory'] = data['SleepHours'].apply(categorize_sleep_hours)\n",
    "\n",
    "    # 20. Categorize Physical Health Days into \"No issues\", \"Mild\", \"Moderate\", and \"Severe\"\n",
    "    def categorize_physical_health_days(days):\n",
    "        if days == 0:\n",
    "            return 0  # No issues\n",
    "        elif 1 <= days <= 7:\n",
    "            return 1  # Mild issues\n",
    "        elif 8 <= days <= 14:\n",
    "            return 2  # Moderate issues\n",
    "        else:\n",
    "            return 3  # Severe issues\n",
    "\n",
    "    data['PhysicalHealthCategory'] = data['PhysicalHealthDays'].apply(categorize_physical_health_days)\n",
    "\n",
    "    # 21. Total days taken off.\n",
    "    data['TotalHealthDays'] = data['PhysicalHealthDays'] + data['MentalHealthDays']\n",
    "\n",
    "    # 22. Create a binary indicator for whether a person had any mental health issues\n",
    "    data[\"HadMentalHealthIssues\"] = (data[\"MentalHealthDays\"] > 0).astype(int)\n",
    "\n",
    "    # 23. BMI × Physical Activity → Assess impact of obesity on exercise participation\n",
    "    data[\"BMI_PhysicalActivity\"] = data[\"BMI\"] * data[\"PhysicalActivities\"].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "    \n",
    "    # 24. BMI × Chronic Conditions → Identifies relationships between obesity and chronic disease burden\n",
    "    data[\"BMI_ChronicConditions\"] = data[\"BMI\"] * data[\"ComorbidityIndex\"]\n",
    "    \n",
    "    # 25. BMI × Sleep Deficit → Captures how sleep disturbances may be associated with weight\n",
    "    data[\"BMI_SleepDeficit\"] = data[\"BMI\"] * data[\"SleepDeficit\"]\n",
    "    \n",
    "    # 26. BMI × Heart Risk Score → Evaluates combined impact of weight and heart-related risks\n",
    "    data[\"BMI_HeartRisk\"] = data[\"BMI\"] * data[\"HeartRiskScore\"]\n",
    "\n",
    "    # 27. Physical Activity × AgeMidpoint → How exercise patterns vary by age\n",
    "    data[\"PhysicalActivity_Age\"] = data[\"PhysicalActivities\"].apply(lambda x: 1 if x == 'Yes' else 0) * data[\"AgeMidpoint\"]\n",
    "    \n",
    "    # 28. Physical Activity × BMI → Relationship between obesity and exercise behavior\n",
    "    data[\"PhysicalActivity_BMI\"] = data[\"PhysicalActivities\"].apply(lambda x: 1 if x == 'Yes' else 0) * data[\"BMI\"]\n",
    "    \n",
    "    # 29. Physical Activity × Chronic Conditions → Effect of chronic illnesses on exercise\n",
    "    data[\"PhysicalActivity_ChronicConditions\"] = data[\"PhysicalActivities\"].apply(lambda x: 1 if x == 'Yes' else 0) * data[\"ComorbidityIndex\"]\n",
    "    \n",
    "    # 30. Physical Activity × Sleep Deficit → Relationship between exercise and sleep habits\n",
    "    data[\"PhysicalActivity_SleepDeficit\"] = data[\"PhysicalActivities\"].apply(lambda x: 1 if x == 'Yes' else 0) * data[\"SleepDeficit\"]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Apply feature engineering to train and test sets\n",
    "X_train = feature_engineering(X_train)\n",
    "X_test = feature_engineering(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:43:41.318811Z",
     "iopub.status.busy": "2025-02-12T17:43:41.318455Z",
     "iopub.status.idle": "2025-02-12T17:44:23.641438Z",
     "shell.execute_reply": "2025-02-12T17:44:23.640251Z",
     "shell.execute_reply.started": "2025-02-12T17:43:41.318782Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Apply feature engineering\n",
    "X_train = feature_engineering(X_train)\n",
    "X_test = feature_engineering(X_test)\n",
    "\n",
    "# Separate data by gender\n",
    "male_data = X_train[X_train['Sex_Male'] == 1]\n",
    "female_data = X_train[X_train['Sex_Male'] == 0]\n",
    "\n",
    "# Find the majority gender\n",
    "max_size = max(len(male_data), len(female_data))\n",
    "\n",
    "# Oversample the minority gender to match the majority size\n",
    "male_data_oversampled = resample(male_data, replace=True, n_samples=max_size, random_state=42)\n",
    "female_data_oversampled = resample(female_data, replace=True, n_samples=max_size, random_state=42)\n",
    "\n",
    "# Combine the balanced dataset\n",
    "X_train_balanced = pd.concat([male_data_oversampled, female_data_oversampled]).sample(frac=1, random_state=42)\n",
    "\n",
    "# Ensure y_train is also updated\n",
    "y_train_balanced = y_train.loc[X_train_balanced.index]\n",
    "\n",
    "# Verify new gender distribution\n",
    "print(\"\\nBalanced Gender Distribution (%):\")\n",
    "print(X_train_balanced['Sex_Male'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the `AgesGrouped` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:44:23.643183Z",
     "iopub.status.busy": "2025-02-12T17:44:23.642817Z",
     "iopub.status.idle": "2025-02-12T17:44:23.709100Z",
     "shell.execute_reply": "2025-02-12T17:44:23.708081Z",
     "shell.execute_reply.started": "2025-02-12T17:44:23.643139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Remove 'AgesGrouped' from X_train and X_test\n",
    "X_train = X_train.drop(columns=['AgesGrouped'])\n",
    "X_test = X_test.drop(columns=['AgesGrouped'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding of the `State` Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:44:23.710210Z",
     "iopub.status.busy": "2025-02-12T17:44:23.709939Z",
     "iopub.status.idle": "2025-02-12T17:44:24.570841Z",
     "shell.execute_reply": "2025-02-12T17:44:24.569877Z",
     "shell.execute_reply.started": "2025-02-12T17:44:23.710184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'State' is a string-type column\n",
    "X_train['State'] = X_train['State'].astype(str)\n",
    "X_test['State'] = X_test['State'].astype(str)\n",
    "\n",
    "# Ensure y_train is a Pandas Series (force it before encoding)\n",
    "y_train = pd.Series(y_train, index=X_train.index, dtype=float)  # Explicitly enforce dtype\n",
    "\n",
    "# Initialize TargetEncoder\n",
    "encoder = TargetEncoder(cols=['State'])\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train['State_encoded'] = encoder.fit_transform(X_train[['State']], y_train.astype(float))  # Ensure float type\n",
    "\n",
    "# Transform test data\n",
    "X_test['State_encoded'] = encoder.transform(X_test[['State']])\n",
    "\n",
    "# Drop original 'State' column\n",
    "X_train.drop(columns=['State'], inplace=True)\n",
    "X_test.drop(columns=['State'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train/Test Data for Tree-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:44:24.572690Z",
     "iopub.status.busy": "2025-02-12T17:44:24.572071Z",
     "iopub.status.idle": "2025-02-12T17:44:24.714227Z",
     "shell.execute_reply": "2025-02-12T17:44:24.713268Z",
     "shell.execute_reply.started": "2025-02-12T17:44:24.572646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Keep raw features for tree-based models\n",
    "X_train_tree = X_train.copy()\n",
    "X_test_tree = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train/Test Data for Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:44:24.715755Z",
     "iopub.status.busy": "2025-02-12T17:44:24.715337Z",
     "iopub.status.idle": "2025-02-12T17:44:24.875561Z",
     "shell.execute_reply": "2025-02-12T17:44:24.874272Z",
     "shell.execute_reply.started": "2025-02-12T17:44:24.715721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train_linear = X_train.copy()\n",
    "X_test_linear = X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate MI scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:44:24.880644Z",
     "iopub.status.busy": "2025-02-12T17:44:24.880287Z",
     "iopub.status.idle": "2025-02-12T17:46:03.817221Z",
     "shell.execute_reply": "2025-02-12T17:46:03.816070Z",
     "shell.execute_reply.started": "2025-02-12T17:44:24.880600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Calculate mutual information\n",
    "mi_scores = mutual_info_classif(X_train_linear, y_train)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "mi_results = pd.DataFrame({\n",
    "    'Feature': X_train_linear.columns,\n",
    "    'MI_Score': mi_scores\n",
    "}).sort_values(by='MI_Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph of the MI scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:46:03.819715Z",
     "iopub.status.busy": "2025-02-12T17:46:03.819360Z",
     "iopub.status.idle": "2025-02-12T17:46:06.099481Z",
     "shell.execute_reply": "2025-02-12T17:46:06.098304Z",
     "shell.execute_reply.started": "2025-02-12T17:46:03.819685Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the number of features dynamically\n",
    "num_features = len(mi_results)\n",
    "\n",
    "# Generate colors using a colormap\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_features))  # Adjust 'viridis' as needed\n",
    "\n",
    "# Plot bar chart with a colormap\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.barh(mi_results['Feature'], mi_results['MI_Score'], color=colors)\n",
    "plt.xlabel(\"Mutual Information Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(f\"Mutual Information Ranked\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "plt.savefig('MI_scores.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:46:06.101072Z",
     "iopub.status.busy": "2025-02-12T17:46:06.100755Z",
     "iopub.status.idle": "2025-02-12T17:46:06.515328Z",
     "shell.execute_reply": "2025-02-12T17:46:06.514121Z",
     "shell.execute_reply.started": "2025-02-12T17:46:06.101043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(y=mi_results['MI_Score'])\n",
    "plt.title(\"Distribution of Mutual Information Scores\")\n",
    "plt.xlabel(\"Mutual Information Score\")\n",
    "plt.savefig('dist_of_MI.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:46:06.516788Z",
     "iopub.status.busy": "2025-02-12T17:46:06.516399Z",
     "iopub.status.idle": "2025-02-12T17:46:09.162781Z",
     "shell.execute_reply": "2025-02-12T17:46:09.161523Z",
     "shell.execute_reply.started": "2025-02-12T17:46:06.516758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compute correlation matrix (only for numerical features)\n",
    "corr_matrix = X_train_linear.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:46:09.164050Z",
     "iopub.status.busy": "2025-02-12T17:46:09.163741Z",
     "iopub.status.idle": "2025-02-12T17:46:09.197820Z",
     "shell.execute_reply": "2025-02-12T17:46:09.196641Z",
     "shell.execute_reply.started": "2025-02-12T17:46:09.164026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Identify features with high correlation (absolute value > 0.8)\n",
    "threshold = 0.8\n",
    "high_corr_pairs = (corr_matrix.abs() > threshold).stack()\n",
    "high_corr_pairs = high_corr_pairs[high_corr_pairs == True].reset_index()\n",
    "high_corr_pairs.columns = ['Feature1', 'Feature2', 'Correlation']\n",
    "\n",
    "# Remove self-correlations (where Feature1 == Feature2)\n",
    "high_corr_pairs = high_corr_pairs[high_corr_pairs['Feature1'] != high_corr_pairs['Feature2']]\n",
    "\n",
    "# Ensure unique sets of correlated pairs by sorting feature names and dropping duplicates\n",
    "high_corr_pairs['Sorted_Pair'] = high_corr_pairs.apply(lambda row: tuple(sorted([row['Feature1'], row['Feature2']])), axis=1)\n",
    "high_corr_pairs = high_corr_pairs.drop_duplicates(subset=['Sorted_Pair']).drop(columns=['Sorted_Pair'])\n",
    "\n",
    "# Add a column that shows the correlation score of Feature1 with Feature2\n",
    "high_corr_pairs['Feature1_Correlation'] = high_corr_pairs.apply(\n",
    "    lambda row: corr_matrix.loc[row['Feature1'], row['Feature2']], axis=1\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Unique highly correlated feature pairs with correlation scores:\")\n",
    "print(high_corr_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Lower MI Features in Correlated Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:46:09.199083Z",
     "iopub.status.busy": "2025-02-12T17:46:09.198827Z",
     "iopub.status.idle": "2025-02-12T17:46:09.558503Z",
     "shell.execute_reply": "2025-02-12T17:46:09.557560Z",
     "shell.execute_reply.started": "2025-02-12T17:46:09.199061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Iterate through correlated feature pairs and remove the one with the lowest MI score\n",
    "for _, row in high_corr_pairs.iterrows():\n",
    "    feature1, feature2 = row['Feature1'], row['Feature2']\n",
    "    \n",
    "    # Ensure both features exist in mi_results before proceeding\n",
    "    if feature1 in mi_results['Feature'].values and feature2 in mi_results['Feature'].values:\n",
    "        # Retrieve MI scores for the correlated features\n",
    "        mi_score1 = mi_results.loc[mi_results['Feature'] == feature1, 'MI_Score'].values[0]\n",
    "        mi_score2 = mi_results.loc[mi_results['Feature'] == feature2, 'MI_Score'].values[0]\n",
    "\n",
    "        # Identify the feature with the lower MI score\n",
    "        feature_to_drop = feature1 if mi_score1 < mi_score2 else feature2\n",
    "\n",
    "        # Drop from X_train_linear and X_test_linear if it exists\n",
    "        if feature_to_drop in X_train_linear.columns:\n",
    "            X_train_linear = X_train_linear.drop(columns=[feature_to_drop])\n",
    "        if feature_to_drop in X_test_linear.columns:\n",
    "            X_test_linear = X_test_linear.drop(columns=[feature_to_drop])\n",
    "        \n",
    "        # Drop from mi_results if it exists\n",
    "        mi_results = mi_results[mi_results['Feature'] != feature_to_drop]\n",
    "\n",
    "        print(f\"Dropped feature: {feature_to_drop} (Lower MI Score: {min(mi_score1, mi_score2)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Remaining Features by MI Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:46:09.559675Z",
     "iopub.status.busy": "2025-02-12T17:46:09.559403Z",
     "iopub.status.idle": "2025-02-12T17:46:11.742521Z",
     "shell.execute_reply": "2025-02-12T17:46:11.741382Z",
     "shell.execute_reply.started": "2025-02-12T17:46:09.559652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the number of features dynamically\n",
    "num_features = len(mi_results)\n",
    "\n",
    "# Generate colors using a colormap\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_features))  # Adjust 'viridis' as needed\n",
    "\n",
    "# Plot bar chart with a colormap\n",
    "plt.figure(figsize=(10, 20))\n",
    "plt.barh(mi_results['Feature'], mi_results['MI_Score'], color=colors)\n",
    "plt.xlabel(\"Mutual Information Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(f\"Mutual Information Ranked\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "plt.savefig('MI_scores_2.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the  graph:\n",
    "\n",
    "1. Retain top 10-15 features with the highest MI scores for your initial model.\n",
    "2. Use recursive feature elimination (RFE) or a model-specific feature importance metric (e.g., feature importance from Random Forest) to fine-tune the selection further.\n",
    "3. Consider the importance of domain knowledge for including some features, even if their MI scores are slightly lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Linear Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:46:11.744034Z",
     "iopub.status.busy": "2025-02-12T17:46:11.743728Z",
     "iopub.status.idle": "2025-02-12T17:46:11.767286Z",
     "shell.execute_reply": "2025-02-12T17:46:11.765803Z",
     "shell.execute_reply.started": "2025-02-12T17:46:11.744008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensure top_features contains the top 15 features\n",
    "# top_n = 15\n",
    "# top_features = mi_results.head(top_n)['Feature'].tolist()  # Convert Series to list\n",
    "\n",
    "# Features based on domain knowledge \n",
    "top_features = [\n",
    "    'HadAngina',\n",
    "    'HeartRiskScore',\n",
    "    'PhysicalActivities',\n",
    "    'SleepCategory',\n",
    "    'LastCheckupTime',\n",
    "    'ChestScan',\n",
    "    'FluVaxLast12',\n",
    "    'GeneralHealth',\n",
    "    'PneumoVaxEver',\n",
    "    'AgeMidpoint',\n",
    "    'SmokerStatus',\n",
    "]\n",
    "\n",
    "# Filter X_train_linear and X_test_linear to include only top features\n",
    "X_train_linear = X_train_linear[top_features]\n",
    "X_test_linear = X_test_linear[top_features]\n",
    "\n",
    "# Verify the selection\n",
    "print(f\"Selected top 15 features:\", top_features)\n",
    "print(\"X_train_linear shape:\", X_train_linear.shape)\n",
    "print(\"X_test_linear shape:\", X_test_linear.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling and SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:46:11.768734Z",
     "iopub.status.busy": "2025-02-12T17:46:11.768381Z",
     "iopub.status.idle": "2025-02-12T17:46:11.787677Z",
     "shell.execute_reply": "2025-02-12T17:46:11.786479Z",
     "shell.execute_reply.started": "2025-02-12T17:46:11.768706Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display class distribution as percentages\n",
    "heart_attack_distribution = df['HadHeartAttack'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print results\n",
    "print(\"Percentage distribution of HadHeartAttack:\")\n",
    "print(heart_attack_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T17:46:11.789010Z",
     "iopub.status.busy": "2025-02-12T17:46:11.788736Z",
     "iopub.status.idle": "2025-02-12T17:57:02.414726Z",
     "shell.execute_reply": "2025-02-12T17:57:02.413555Z",
     "shell.execute_reply.started": "2025-02-12T17:46:11.788987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Initialize SMOTETomek\n",
    "smote_tomek = SMOTETomek(random_state=0)\n",
    "\n",
    "# Apply SMOTETomek separately for linear and tree-based models\n",
    "X_train_linear, y_train_linear = smote_tomek.fit_resample(X_train_linear, y_train)\n",
    "\n",
    "X_train_tree, y_train_tree = smote_tomek.fit_resample(X_train_tree, y_train)\n",
    "\n",
    "# Verify class distributions\n",
    "print(\"\\nClass distribution after SMOTE-Tomek (Linear Model Data):\\n\", y_train_linear.value_counts())\n",
    "print(\"\\nClass distribution after SMOTE-Tomek (Tree Model Data):\\n\", y_train_tree.value_counts())\n",
    "\n",
    "# Verify new shapes of training data\n",
    "print(f\"X_train_linear shape: {X_train_linear.shape}\")\n",
    "print(f\"X_train_tree shape: {X_train_tree.shape}\")\n",
    "print(f\"y_train_linear shape: {y_train_linear.shape}\")\n",
    "print(f\"y_train_tree shape: {y_train_tree.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Tree-Based Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T18:56:30.023640Z",
     "iopub.status.busy": "2025-02-12T18:56:30.023055Z",
     "iopub.status.idle": "2025-02-12T18:57:13.697983Z",
     "shell.execute_reply": "2025-02-12T18:57:13.696867Z",
     "shell.execute_reply.started": "2025-02-12T18:56:30.023579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a Random Forest model to get feature importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "rf.fit(X_train_tree, y_train_tree)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train_tree.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display top 15 features\n",
    "top_n = 15\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_importance.head(top_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T18:57:29.339234Z",
     "iopub.status.busy": "2025-02-12T18:57:29.338786Z",
     "iopub.status.idle": "2025-02-12T18:57:29.374710Z",
     "shell.execute_reply": "2025-02-12T18:57:29.373575Z",
     "shell.execute_reply.started": "2025-02-12T18:57:29.339195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the top 10 features for tree-based models\n",
    "top_10_features = [\n",
    "    \"BMI_HeartRisk\", \"HeartHealthInteraction\", \"HeartRiskScore\", \n",
    "    \"AgeMidpoint\", \"HadAngina\", \"HeartSleepInteraction\", \n",
    "    \"DifficultyWalking\", \"GeneralHealth\", \"PhysicalHealthDays\",\n",
    "    \"TetanusLast10Tdap\"  # Replacing \"AlcoholDrinkers\" and \"HeightInMeters\"\n",
    "]\n",
    "\n",
    "# Select only these features for training and testing sets\n",
    "X_train_tree = X_train_tree[top_10_features]\n",
    "X_test_tree = X_test_tree[top_10_features]\n",
    "\n",
    "# Verify new feature count\n",
    "print(f\"Reduced X_train_tree shape: {X_train_tree.shape}\")\n",
    "print(f\"Reduced X_test_tree shape: {X_test_tree.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Data for Linear Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T18:57:48.145812Z",
     "iopub.status.busy": "2025-02-12T18:57:48.145426Z",
     "iopub.status.idle": "2025-02-12T18:57:48.219600Z",
     "shell.execute_reply": "2025-02-12T18:57:48.218354Z",
     "shell.execute_reply.started": "2025-02-12T18:57:48.145780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize and fit the scaler on X_train_linear\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_linear)  # Fit + transform on X_train_linear\n",
    "\n",
    "# Apply the SAME scaler to X_test_linear (only transform, no fitting!)\n",
    "X_test_scaled = scaler.transform(X_test_linear)\n",
    "\n",
    "# Convert back to DataFrame (optional, if you want to keep column names)\n",
    "X_train_linear = pd.DataFrame(X_train_scaled, columns=X_train_linear.columns, index=X_train_linear.index)\n",
    "X_test_linear = pd.DataFrame(X_test_scaled, columns=X_test_linear.columns, index=X_test_linear.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#5E6997\">Predict Heart Attacks</span> <a class=\"anchor\" id=\"predict\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T18:57:51.971685Z",
     "iopub.status.busy": "2025-02-12T18:57:51.971308Z",
     "iopub.status.idle": "2025-02-12T18:57:51.981380Z",
     "shell.execute_reply": "2025-02-12T18:57:51.980189Z",
     "shell.execute_reply.started": "2025-02-12T18:57:51.971645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model_performance(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates model performance, calculates ROC score, plots the confusion matrix, \n",
    "    and saves it with a dynamic file name based on the model's class name.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model object (supports `predict_proba`, `decision_function`, or TensorFlow's `predict`).\n",
    "    - X_test: Test feature set.\n",
    "    - y_test: Test target set.\n",
    "    \"\"\"\n",
    "    # Determine the model name\n",
    "    if hasattr(model, \"__class__\"):\n",
    "        model_name = model.__class__.__name__.lower()\n",
    "    else:\n",
    "        model_name = \"tensorflow_model\"\n",
    "\n",
    "    # Handle prediction probabilities or outputs\n",
    "    try:\n",
    "        if hasattr(model, \"predict_proba\"):  # Scikit-learn models with predict_proba\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        elif hasattr(model, \"decision_function\"):  # Scikit-learn models with decision_function\n",
    "            y_proba = model.decision_function(X_test)\n",
    "        elif hasattr(model, \"predict\"):  # TensorFlow models\n",
    "            y_proba = model.predict(X_test).ravel()  # Flatten probabilities for binary classification\n",
    "        else:\n",
    "            raise AttributeError(f\"The model {model_name} does not support the necessary prediction methods.\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error in generating predictions: {e}\")\n",
    "\n",
    "    # Convert probabilities to binary predictions for confusion matrix and classification report\n",
    "    y_pred = (y_proba >= 0.5).astype(int)\n",
    "\n",
    "    # Compute ROC AUC Score\n",
    "    roc_score = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"\\nROC AUC Score: {roc_score:.3f}\")\n",
    "\n",
    "    # Evaluate the model with confusion matrix and classification report\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, zero_division=1)\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['No Stroke', 'Stroke'])\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))  # Adjust heatmap size here\n",
    "    disp.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "    plt.title(\"Confusion Matrix Heatmap\")\n",
    "\n",
    "    # Save the confusion matrix heatmap with dynamic name\n",
    "    cm_filename = f\"cmatrix_{model_name}.png\"\n",
    "    plt.savefig(cm_filename, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "    print('-' * 55 + '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-Based Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T18:57:56.698971Z",
     "iopub.status.busy": "2025-02-12T18:57:56.698558Z",
     "iopub.status.idle": "2025-02-12T18:58:08.298089Z",
     "shell.execute_reply": "2025-02-12T18:58:08.296672Z",
     "shell.execute_reply.started": "2025-02-12T18:57:56.698941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# List of tree-based models (Now using XGBoost and LightGBM)\n",
    "tree_models = [\n",
    "    XGBClassifier(random_state=0, n_estimators=200, learning_rate=0.1, max_depth=5),\n",
    "    LGBMClassifier(random_state=0, n_estimators=200, learning_rate=0.1, max_depth=5)\n",
    "]\n",
    "\n",
    "# Train and evaluate tree-based models\n",
    "print(\"\\n🌲 Training Tree-Based Models:\")\n",
    "for model in tree_models:\n",
    "    model.fit(X_train_tree, y_train_tree)\n",
    "    print(f\"Evaluating {model.__class__.__name__}...\")\n",
    "    evaluate_model_performance(model, X_test_tree, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGBoost Results**\\\n",
    "ROC AUC Score: 0.860\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.97      0.97     46518\n",
    "           1       0.45      0.37      0.40      2687\n",
    "\n",
    "    accuracy                           0.94     49205\n",
    "    macro avg      0.71      0.67      0.69     49205\n",
    "    weighted avg   0.94      0.94      0.94     49205\n",
    "\n",
    "**LightGBM Results**\\\n",
    "ROC AUC Score: 0.864\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.98      0.97     46518\n",
    "           1       0.46      0.32      0.38      2687\n",
    "\n",
    "    accuracy                           0.94     49205\n",
    "    macro avg      0.71      0.65      0.67     49205\n",
    "    weighted avg   0.93      0.94      0.94     49205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T18:59:15.984832Z",
     "iopub.status.busy": "2025-02-12T18:59:15.984005Z",
     "iopub.status.idle": "2025-02-12T18:59:41.885381Z",
     "shell.execute_reply": "2025-02-12T18:59:41.884339Z",
     "shell.execute_reply.started": "2025-02-12T18:59:15.984797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# List of linear models\n",
    "linear_models = [\n",
    "    LogisticRegression(max_iter=1000, random_state=0),\n",
    "    LinearSVC(max_iter=100000, random_state=0)\n",
    "]\n",
    "\n",
    "# Train and evaluate linear models\n",
    "print(\"\\n📈 Training Linear Models:\")\n",
    "for model in linear_models:\n",
    "    model.fit(X_train_linear, y_train_linear)\n",
    "    print(f\"Evaluating {model.__class__.__name__}...\")\n",
    "    evaluate_model_performance(model, X_test_linear, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Results**\\\n",
    "ROC AUC Score: 0.875\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.82      0.90     46518\n",
    "           1       0.20      0.75      0.31      2687\n",
    "\n",
    "    accuracy                           0.82     49205\n",
    "    macro avg      0.59      0.79      0.60     49205\n",
    "    weighted avg   0.94      0.82      0.86     49205\n",
    "\n",
    "\\\n",
    "**SVM Results**\\\n",
    "ROC AUC Score: 0.874\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      0.95      0.96     46518\n",
    "           1       0.39      0.51      0.44      2687\n",
    "\n",
    "    accuracy                           0.93     49205\n",
    "    macro avg      0.68      0.73      0.70     49205\n",
    "    weighted avg   0.94      0.93      0.93     49205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T18:59:55.384222Z",
     "iopub.status.busy": "2025-02-12T18:59:55.383810Z",
     "iopub.status.idle": "2025-02-12T19:03:14.359564Z",
     "shell.execute_reply": "2025-02-12T19:03:14.358440Z",
     "shell.execute_reply.started": "2025-02-12T18:59:55.384190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define a TensorFlow model\n",
    "def create_tensorflow_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),  # Explicit Input layer\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Initialize and train the TensorFlow model\n",
    "tensorflow_model = create_tensorflow_model(X_train_linear.shape[1])\n",
    "tensorflow_model.fit(X_train_linear, y_train_linear, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the TensorFlow model\n",
    "loss, accuracy = tensorflow_model.evaluate(X_test_linear, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "evaluate_model_performance(tensorflow_model, X_test_linear, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensorflow Results**\\\n",
    "ROC AUC Score: 0.873\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.90      0.94     46518\n",
    "           1       0.27      0.65      0.38      2687\n",
    "\n",
    "    accuracy                           0.89     49205\n",
    "    macro avg      0.62      0.77      0.66     49205\n",
    "    weighted avg   0.94      0.89      0.91     49205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How I Will Improving Models in the Fine-Tuning Process:\n",
    "1. **Hyperparameter Tuning**: Use GridSearchCV to optimize hyperparameters for each model.      \n",
    "2. **Cross-Validation**: Use cross-validation (cross_val_score) to ensure robust evaluation and avoid overfitting to the train-test split.\n",
    "\n",
    "3. **Ensemble Methods**: Combine multiple models using VotingClassifier or StackingClassifier to leverage the strengths of individual models.\n",
    "\n",
    "4. **Feature Engineering**: Analyze and transform features to extract more predictive information (e.g., polynomial features, interactions, feature selection).\n",
    "\n",
    "5. **Threshold Tuning**: Adjust the classification threshold for each model to balance precision and recall.\n",
    "\n",
    "6. **Tensorflow Model**: Increase model capacity, experiment with layer types, optimize training, and regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-13T01:13:41.095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Compute class imbalance ratio for scale_pos_weight\n",
    "class_weight = len(y_train_tree[y_train_tree == 0]) / len(y_train_tree[y_train_tree == 1])\n",
    "\n",
    "# Define parameter grid for Grid Search (smaller range to avoid excessive computations)\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 150, 200],  # Moderate number of boosting rounds\n",
    "    'learning_rate': [0.03, 0.05, 0.08],  # More gradual learning\n",
    "    'max_depth': [3, 4],  # Lower depth to reduce overfitting\n",
    "    'subsample': [0.6, 0.7, 0.8],  # Introduce randomness\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],  # Use subset of features per tree\n",
    "    'min_child_weight': [3, 5, 7],  # Require more samples per split to prevent overfitting\n",
    "    'reg_alpha': [0.0, 0.1, 0.5],  # L1 regularization (sparsity)\n",
    "    'reg_lambda': [0.5, 1.0, 2.0]  # L2 regularization (shrinkage)\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize XGBoost with class balancing\n",
    "xgb = XGBClassifier(scale_pos_weight=class_weight, random_state=0)\n",
    "\n",
    "# Use Stratified K-Folds for better validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Perform Grid Search for hyperparameter tuning\n",
    "tuned_xgb = GridSearchCV(\n",
    "    xgb, \n",
    "    param_grid_xgb, \n",
    "    scoring='roc_auc', \n",
    "    cv=skf,  # Use stratified K-Folds\n",
    "    n_jobs=-1, \n",
    "    verbose=1  # Show progress\n",
    ")\n",
    "\n",
    "# Train the model with tuned parameters\n",
    "tuned_xgb.fit(X_train_tree, y_train_tree)\n",
    "\n",
    "# Print best parameters and performance\n",
    "print(\"Best parameters for XGBoost:\", tuned_xgb.best_params_)\n",
    "print(f\"Best ROC AUC for XGBoost: {tuned_xgb.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "print(\"\\nTEST DATA RESULTS\")\n",
    "evaluate_model_performance(tuned_xgb, X_test_tree, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters for XGBoost: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.8}\\\n",
    "Best ROC AUC for XGBoost: 0.98952792537362\n",
    "\n",
    "TEST DATA RESULTS\n",
    "\n",
    "ROC AUC Score: 0.862\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.97      0.97     46518\n",
    "           1       0.45      0.37      0.41      2687\n",
    "\n",
    "    accuracy                           0.94     49205\n",
    "    macro avg      0.71      0.67      0.69     49205\n",
    "    weighted avg   0.94      0.94      0.94     49205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-13T01:13:41.095Z",
     "iopub.execute_input": "2025-02-12T19:27:07.622264Z",
     "iopub.status.busy": "2025-02-12T19:27:07.621908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import gc\n",
    "\n",
    "# Free up memory before running Grid Search\n",
    "gc.collect()\n",
    "\n",
    "# Define optimized parameter grid for LightGBM\n",
    "param_tuned_lgb = {\n",
    "    'n_estimators': [100, 150, 200],  # Slightly limit number of trees\n",
    "    'learning_rate': [0.05, 0.08, 0.1],  # Adjust learning rate for more stability\n",
    "    'max_depth': [3, 4, 5],  # Prevent deep trees that overfit\n",
    "    'subsample': [0.6, 0.7, 0.8],  # Increase randomness in boosting\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],  # Random feature selection to improve generalization\n",
    "    'reg_alpha': [0.0, 0.1, 0.5],  # L1 regularization to remove irrelevant features\n",
    "    'reg_lambda': [0.5, 1.0, 2.0],  # L2 regularization to shrink coefficients\n",
    "    'min_child_samples': [10, 20, 30]  # Require more samples before splitting nodes\n",
    "}\n",
    "\n",
    "# Initialize model (Turn off LightGBM logs)\n",
    "lgb = LGBMClassifier(random_state=0, n_jobs=-1, verbose=-1)\n",
    "\n",
    "# Grid search with LightGBM (Silencing output)\n",
    "tuned_lgb = GridSearchCV(\n",
    "    lgb, \n",
    "    param_tuned_lgb, \n",
    "    scoring='roc_auc', \n",
    "    cv=5, \n",
    "    n_jobs=-1,  \n",
    "    verbose=0  # Silence GridSearchCV output\n",
    ")\n",
    "\n",
    "# Train the model (WITHOUT early stopping, NO verbose logging)\n",
    "tuned_lgb.fit(X_train_tree, y_train_tree)\n",
    "\n",
    "# Print best parameters and performance\n",
    "print(\"Best parameters for LightGBM:\", tuned_lgb.best_params_)\n",
    "print(f\"Best ROC AUC for LightGBM: {tuned_lgb.best_score_:.3f}\")\n",
    "\n",
    "# **Manually refit best model WITHOUT logging output**\n",
    "best_lgb = LGBMClassifier(**tuned_lgb.best_params_, random_state=0, n_jobs=-1, verbose=-1)\n",
    "\n",
    "# Train best model normally (NO logging)\n",
    "best_lgb.fit(X_train_tree, y_train_tree)\n",
    "\n",
    "# Evaluate the final model\n",
    "print(\"\\nTEST DATA RESULTS\")\n",
    "evaluate_model_performance(best_lgb, X_test_tree, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuned LightGBM Results**\\\n",
    "Best parameters for LightGBM: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.7}\\\n",
    "Best ROC AUC for LightGBM: 0.9885577254576073\n",
    "\n",
    "TEST DATA RESULTS\n",
    "\n",
    "ROC AUC Score: 0.864\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.96      0.98      0.97     46518\n",
    "           1       0.46      0.32      0.38      2687\n",
    "\n",
    "    accuracy                           0.94     49205\n",
    "    macro avg      0.71      0.65      0.67     49205\n",
    "    weighted avg   0.93      0.94      0.94     49205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:15:21.889283Z",
     "iopub.status.busy": "2025-02-12T19:15:21.888881Z",
     "iopub.status.idle": "2025-02-12T19:16:59.309918Z",
     "shell.execute_reply": "2025-02-12T19:16:59.308606Z",
     "shell.execute_reply.started": "2025-02-12T19:15:21.889252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define expanded parameter grid for Logistic Regression\n",
    "param_tuned_lr = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],  # Wider range of regularization strengths\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],  # No 'None'\n",
    "    'solver': ['liblinear', 'lbfgs', 'saga', 'newton-cg'],  \n",
    "    'max_iter': [1000, 5000]  # Higher iterations for convergence\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "lr = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "\n",
    "# Grid search\n",
    "tuned_lr = GridSearchCV(lr, param_tuned_lr, scoring='roc_auc', cv=5, n_jobs=-1)\n",
    "tuned_lr.fit(X_train_linear, y_train_linear)\n",
    "\n",
    "print(\"Best parameters for Logistic Regression:\", tuned_lr.best_params_)\n",
    "print(f\"Best ROC AUC for Logistic Regression: {tuned_lr.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nTEST DATA RESULTS\")\n",
    "evaluate_model_performance(tuned_lr, X_test_linear, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuned Logistic Regression Test Results**\\\n",
    "Best parameters for Logistic Regression: {'C': 0.01, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'liblinear'}\\\n",
    "Best ROC AUC for Logistic Regression: 0.8815435924922415\n",
    "\n",
    "TEST DATA RESULTS\n",
    "\n",
    "ROC AUC Score: 0.875\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.98      0.82      0.90     46518\n",
    "           1       0.20      0.75      0.31      2687\n",
    "\n",
    "    accuracy                           0.82     49205\n",
    "    macro avg      0.59      0.79      0.60     49205\n",
    "    weighted avg   0.94      0.82      0.86     49205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T19:16:59.312599Z",
     "iopub.status.busy": "2025-02-12T19:16:59.312251Z",
     "iopub.status.idle": "2025-02-12T19:26:57.891767Z",
     "shell.execute_reply": "2025-02-12T19:26:57.890285Z",
     "shell.execute_reply.started": "2025-02-12T19:16:59.312571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Expanded parameter grid\n",
    "param_tuned_svm = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Wider range of regularization strengths\n",
    "    'loss': ['hinge', 'squared_hinge'],  # Different loss functions\n",
    "    'dual': [True, False]  # Choose best solver for dataset size\n",
    "}\n",
    "\n",
    "# Initialize model\n",
    "svm = LinearSVC(max_iter=10000, random_state=0, class_weight='balanced')\n",
    "\n",
    "# Grid search with cross-validation\n",
    "tuned_svm = GridSearchCV(svm, param_tuned_svm, scoring='roc_auc', cv=5, n_jobs=-1)\n",
    "tuned_svm.fit(X_train_linear, y_train_linear)\n",
    "\n",
    "print(\"Best parameters for Linear SVM:\", tuned_svm.best_params_)\n",
    "print(f\"Best ROC AUC for Linear SVM: {tuned_svm.best_score_:.3f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nTEST DATA RESULTS\")\n",
    "evaluate_model_performance(tuned_svm, X_test_linear, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuned SVM Test Results**\\\n",
    "Best parameters for Linear SVM: {'C': 0.01}\\\n",
    "Best ROC AUC for Linear SVM: 0.8811656993247372\n",
    "\n",
    "TEST DATA RESULTS\n",
    "\n",
    "ROC AUC Score: 0.874\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.97      0.95      0.96     46518\n",
    "           1       0.39      0.51      0.44      2687\n",
    "\n",
    "    accuracy                           0.93     49205\n",
    "    macro avg      0.68      0.73      0.70     49205\n",
    "    weighted avg   0.94      0.93      0.93     49205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned Tensorflow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Hyperparameter Tuning Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-12T19:26:57.892498Z",
     "iopub.status.idle": "2025-02-12T19:26:57.892833Z",
     "shell.execute_reply": "2025-02-12T19:26:57.892705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras  # ✅ This ensures Keras is properly defined\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Define a function to build the model with tunable hyperparameters\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Input layer (match input shape to X_train_linear features)\n",
    "    model.add(keras.layers.Input(shape=(X_train_linear.shape[1],)))\n",
    "\n",
    "    # Tune the number of layers (between 1 and 3 hidden layers)\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(keras.layers.Dense(\n",
    "            units=hp.Int('units_' + str(i), min_value=32, max_value=256, step=32),\n",
    "            activation='relu'\n",
    "        ))\n",
    "\n",
    "        # Add dropout (prevents overfitting)\n",
    "        model.add(keras.layers.Dropout(hp.Float('dropout_rate_' + str(i), 0.1, 0.5, step=0.1)))\n",
    "\n",
    "    # Output layer (Binary classification → Sigmoid activation)\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice('learning_rate', [0.001, 0.01, 0.1])\n",
    "        ),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Hyperparameter Tuning Using Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-12T19:26:57.894249Z",
     "iopub.status.idle": "2025-02-12T19:26:57.894738Z",
     "shell.execute_reply": "2025-02-12T19:26:57.894511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from keras_tuner.tuners import GridSearch\n",
    "\n",
    "# Initialize Keras Tuner with GridSearch\n",
    "tuner = GridSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',  # Optimize for validation accuracy\n",
    "    max_trials=50,  # Total number of grid search trials (depends on the parameter grid)\n",
    "    executions_per_trial=1,  # Number of times to train each model\n",
    "    directory='keras_tuner_results',\n",
    "    project_name='grid_tuned_nn'\n",
    ")\n",
    "\n",
    "# Perform the hyperparameter search\n",
    "tuner.search(X_train_linear, y_train_linear, epochs=10, validation_split=0.2, batch_size=32)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters for TensorFlow Model:\")\n",
    "for param in best_hps.values:\n",
    "    print(param, \":\", best_hps.get(param))\n",
    "\n",
    "# Build the best model with optimal hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the best model\n",
    "history = best_model.fit(X_train_linear, y_train_linear, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = best_model.evaluate(X_test_linear, y_test)\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "\n",
    "# Save the best model\n",
    "best_model.save(\"best_grid_tuned_nn.h5\")\n",
    "\n",
    "print(\"\\nTEST DATA RESULTS\")\n",
    "evaluate_model_performance(best_model, X_test_linear, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-02-12T19:26:57.895644Z",
     "iopub.status.idle": "2025-02-12T19:26:57.896109Z",
     "shell.execute_reply": "2025-02-12T19:26:57.895906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build the best model with optimal hyperparameters\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Train the model\n",
    "history = best_model.fit(\n",
    "    X_train_linear, y_train_linear,\n",
    "    epochs=50,  # Increased to 50 epochs\n",
    "    batch_size=32,  # Keeps batch size at 32\n",
    "    validation_split=0.2,  # 20% validation set\n",
    "    verbose=1  # Displays training progress\n",
    ")\n",
    "# Evaluate on test data\n",
    "test_loss, test_acc = best_model.evaluate(X_test_linear, y_test)\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "\n",
    "# Save the best model\n",
    "best_model.save(\"best_tuned_nn.h5\")\n",
    "\n",
    "print(\"\\nTEST DATA RESULTS\")\n",
    "evaluate_model_performance(best_model, X_test_linear, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuned Tensorflow Model Results**\\\n",
    "Best Hyperparameters for TensorFlow Model:\n",
    "num_layers : 1,\n",
    "units_0 : 64,\n",
    "dropout_rate_0 : 0.5,\n",
    "learning_rate : 0.1\n",
    "\n",
    "ROC AUC Score: 0.821\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.73      0.84     46518\n",
    "           1       0.15      0.83      0.25      2687\n",
    "\n",
    "    accuracy                           0.74     49205\n",
    "    macro avg      0.57      0.78      0.55     49205\n",
    "    weighted avg   0.94      0.74      0.81     49205\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#5E6997\">Conclusion</span> <a class=\"anchor\" id=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1936563,
     "sourceId": 6674905,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
